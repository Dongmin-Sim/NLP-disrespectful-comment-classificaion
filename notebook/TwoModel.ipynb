{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32d76215-45a0-45d2-9c22-d8238dc25cea",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 비매너 댓글 식별"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934ffe9b-b212-4aa8-901d-d8bdf10f10ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. 라이브러리 로드 & 환경설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41a23650-95da-4646-9cf1-d8df1db5d148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "from attrdict import AttrDict\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import *\n",
    "from torch.optim import Adam, AdamW\n",
    "\n",
    "from transformers import logging, get_linear_schedule_with_warmup\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizer,  \n",
    "    AutoTokenizer,\n",
    "    ElectraTokenizer,\n",
    "    AlbertTokenizer\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    BertModel,\n",
    "    AutoModel, \n",
    "    ElectraForSequenceClassification,\n",
    "    BertForSequenceClassification,\n",
    "    AlbertForSequenceClassification\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ac8a70-0f98-4d0b-aa86-782fb48e6f4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1-1 DEVICE 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6de9ab4-71f9-4c71-9d54-0f9a4ab915db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of GPUs:  1\n",
      "Does GPU exist? :  True\n"
     ]
    }
   ],
   "source": [
    "# 사용할 GPU 지정\n",
    "print(\"number of GPUs: \", torch.cuda.device_count())\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Does GPU exist? : \", use_cuda)\n",
    "DEVICE = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb08801-2fdc-446d-8ac0-f182ee2c40fd",
   "metadata": {},
   "source": [
    "### 1-2 DEBUG 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6259836-05cf-47c1-994d-b65f9cb7d89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True 일 때 코드를 실행하면 example 등을 보여줌\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13704a5-e182-4fcb-bd1c-4d4469875485",
   "metadata": {},
   "source": [
    "### Config 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffd75bc6-0fe8-4e05-83d6-e87af303ee3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config/config_model2.json\n",
      "config file loaded\n",
      "bias :  monologg/koelectra-base-v3-bias\n",
      "hate :  beomi/beep-KcELECTRA-base-hate\n"
     ]
    }
   ],
   "source": [
    "config_path = os.path.join('config', 'config_model2.json')\n",
    "print(config_path)\n",
    "\n",
    "# config 적용 \n",
    "def set_config(config_path):\n",
    "    if os.path.lexists(config_path):\n",
    "        with open(config_path, 'r') as f:\n",
    "            args = AttrDict(json.load(f))\n",
    "            \n",
    "            print('config file loaded')\n",
    "            print('bias : ', args.pretrained_bias_model)\n",
    "            print('hate : ', args.pretrained_hate_model)\n",
    "    else:\n",
    "        assert False, 'config json file cannot be found.. please check the path again.'\n",
    "        \n",
    "    return args\n",
    "\n",
    "args = set_config(config_path)\n",
    "\n",
    "os.makedirs(args.result_dir, exist_ok=True)\n",
    "os.makedirs(args.config_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e6fefe-6ab1-47c6-87ac-3999ceb1dc91",
   "metadata": {},
   "source": [
    "### load train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f580dbd-0920-4ef0-905a-7f29ec86e96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>comment</th>\n",
       "      <th>bias</th>\n",
       "      <th>hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"'미스터 션샤인' 변요한, 김태리와 같은 양복 입고 학당 방문! 이유는?\"</td>\n",
       "      <td>김태리 정말 연기잘해 진짜</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"[SC현장]\"\"극사실주의 현실♥\"\"…'가장 보통의 연애' 김래원X공효진, 16년만...</td>\n",
       "      <td>공효진 발연기나이질생각이읍던데 왜계속주연일까</td>\n",
       "      <td>none</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"손연재, 리듬체조 학원 선생님 \"\"하고 싶은 일 해서 행복하다\"\"\"</td>\n",
       "      <td>누구처럼 돈만 밝히는 저급인생은 살아가지마시길~~ 행복은 머니순이 아니니깐 작은거에...</td>\n",
       "      <td>others</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"'섹션TV' 김해숙 \"\"'허스토리' 촬영 후 우울증 얻었다\"\"\"</td>\n",
       "      <td>일본 축구 져라</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"[단독] 임현주 아나운서 “‘노브라 챌린지’ 방송 덕에 낸 용기, 자연스런 논의의...</td>\n",
       "      <td>난 절대로 임현주 욕하는인간이랑은 안논다 @.@</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0         \"'미스터 션샤인' 변요한, 김태리와 같은 양복 입고 학당 방문! 이유는?\"   \n",
       "1  \"[SC현장]\"\"극사실주의 현실♥\"\"…'가장 보통의 연애' 김래원X공효진, 16년만...   \n",
       "2             \"손연재, 리듬체조 학원 선생님 \"\"하고 싶은 일 해서 행복하다\"\"\"   \n",
       "3               \"'섹션TV' 김해숙 \"\"'허스토리' 촬영 후 우울증 얻었다\"\"\"   \n",
       "4  \"[단독] 임현주 아나운서 “‘노브라 챌린지’ 방송 덕에 낸 용기, 자연스런 논의의...   \n",
       "\n",
       "                                             comment    bias  hate  \n",
       "0                                     김태리 정말 연기잘해 진짜    none  none  \n",
       "1                           공효진 발연기나이질생각이읍던데 왜계속주연일까    none  hate  \n",
       "2  누구처럼 돈만 밝히는 저급인생은 살아가지마시길~~ 행복은 머니순이 아니니깐 작은거에...  others  hate  \n",
       "3                                           일본 축구 져라    none  none  \n",
       "4                         난 절대로 임현주 욕하는인간이랑은 안논다 @.@    none  none  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = os.path.join(args.data_dir, 'train.csv')\n",
    "train_df = pd.read_csv(train_path, encoding='UTF-8-SIG')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f330662-184e-475e-9459-eb9730f3672c",
   "metadata": {},
   "source": [
    "### load test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9cbb564-bc13-44ae-a6cb-0df444a3df3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>류현경♥︎박성훈, 공개연애 4년차 애정전선 이상無..\"의지 많이 된다\"[종합]</td>\n",
       "      <td>둘다 넘 좋다~행복하세요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\"현금 유도+1인 1라면?\"…'골목식당' 백종원, 초심 잃은 도시락집에 '경악' [종합]</td>\n",
       "      <td>근데 만원이하는 현금결제만 하라고 써놓은집 우리나라에 엄청 많은데</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>입대 D-11' 서은광의 슬픈 멜로디..비투비, 눈물의 첫 체조경기장[콘서트 종합]</td>\n",
       "      <td>누군데 얘네?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>아이콘택트' 리쌍 길, 3년 전 결혼설 부인한 이유 공개…\"결혼,출산 숨겼다\"</td>\n",
       "      <td>쑈 하지마라 짜식아!음주 1번은 실수, 2번은 고의, 3번은 인간쓰레기다.슬금슬금 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>구하라, 안검하수 반박 해프닝...\"당당하다\"vs\"그렇게까지\" 설전 [종합]</td>\n",
       "      <td>안검하수 가지고 있는 분께 희망을 주고 싶은건가요? 수술하면 이렇게 자연스러워진다고...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                              title  \\\n",
       "0   0        류현경♥︎박성훈, 공개연애 4년차 애정전선 이상無..\"의지 많이 된다\"[종합]   \n",
       "1   1  \"현금 유도+1인 1라면?\"…'골목식당' 백종원, 초심 잃은 도시락집에 '경악' [종합]   \n",
       "2   2     입대 D-11' 서은광의 슬픈 멜로디..비투비, 눈물의 첫 체조경기장[콘서트 종합]   \n",
       "3   3        아이콘택트' 리쌍 길, 3년 전 결혼설 부인한 이유 공개…\"결혼,출산 숨겼다\"   \n",
       "4   4         구하라, 안검하수 반박 해프닝...\"당당하다\"vs\"그렇게까지\" 설전 [종합]   \n",
       "\n",
       "                                             comment  \n",
       "0                                      둘다 넘 좋다~행복하세요  \n",
       "1               근데 만원이하는 현금결제만 하라고 써놓은집 우리나라에 엄청 많은데  \n",
       "2                                            누군데 얘네?  \n",
       "3  쑈 하지마라 짜식아!음주 1번은 실수, 2번은 고의, 3번은 인간쓰레기다.슬금슬금 ...  \n",
       "4  안검하수 가지고 있는 분께 희망을 주고 싶은건가요? 수술하면 이렇게 자연스러워진다고...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_path = os.path.join(args.data_dir, 'test.csv')\n",
    "test_df = pd.read_csv(test_path, encoding='UTF-8-SIG')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ff9c96-10d1-4fbe-a07b-1ae0235cc960",
   "metadata": {},
   "source": [
    "# 3. 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea9a6b7-7390-411e-a835-e60fa6732f1b",
   "metadata": {},
   "source": [
    "## 3-1. 라벨값 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a62d9d0-c7d1-4e33-99e5-9acba1e6fd59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>comment</th>\n",
       "      <th>bias</th>\n",
       "      <th>hate</th>\n",
       "      <th>bias_labels</th>\n",
       "      <th>hate_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"'미스터 션샤인' 변요한, 김태리와 같은 양복 입고 학당 방문! 이유는?\"</td>\n",
       "      <td>김태리 정말 연기잘해 진짜</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"[SC현장]\"\"극사실주의 현실♥\"\"…'가장 보통의 연애' 김래원X공효진, 16년만...</td>\n",
       "      <td>공효진 발연기나이질생각이읍던데 왜계속주연일까</td>\n",
       "      <td>none</td>\n",
       "      <td>hate</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"손연재, 리듬체조 학원 선생님 \"\"하고 싶은 일 해서 행복하다\"\"\"</td>\n",
       "      <td>누구처럼 돈만 밝히는 저급인생은 살아가지마시길~~ 행복은 머니순이 아니니깐 작은거에...</td>\n",
       "      <td>others</td>\n",
       "      <td>hate</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"'섹션TV' 김해숙 \"\"'허스토리' 촬영 후 우울증 얻었다\"\"\"</td>\n",
       "      <td>일본 축구 져라</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"[단독] 임현주 아나운서 “‘노브라 챌린지’ 방송 덕에 낸 용기, 자연스런 논의의...</td>\n",
       "      <td>난 절대로 임현주 욕하는인간이랑은 안논다 @.@</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0         \"'미스터 션샤인' 변요한, 김태리와 같은 양복 입고 학당 방문! 이유는?\"   \n",
       "1  \"[SC현장]\"\"극사실주의 현실♥\"\"…'가장 보통의 연애' 김래원X공효진, 16년만...   \n",
       "2             \"손연재, 리듬체조 학원 선생님 \"\"하고 싶은 일 해서 행복하다\"\"\"   \n",
       "3               \"'섹션TV' 김해숙 \"\"'허스토리' 촬영 후 우울증 얻었다\"\"\"   \n",
       "4  \"[단독] 임현주 아나운서 “‘노브라 챌린지’ 방송 덕에 낸 용기, 자연스런 논의의...   \n",
       "\n",
       "                                             comment    bias  hate  \\\n",
       "0                                     김태리 정말 연기잘해 진짜    none  none   \n",
       "1                           공효진 발연기나이질생각이읍던데 왜계속주연일까    none  hate   \n",
       "2  누구처럼 돈만 밝히는 저급인생은 살아가지마시길~~ 행복은 머니순이 아니니깐 작은거에...  others  hate   \n",
       "3                                           일본 축구 져라    none  none   \n",
       "4                         난 절대로 임현주 욕하는인간이랑은 안논다 @.@    none  none   \n",
       "\n",
       "  bias_labels  hate_labels  \n",
       "0           0            0  \n",
       "1           0            1  \n",
       "2           1            1  \n",
       "3           0            0  \n",
       "4           0            0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_labels = ['none', 'others', 'gender']\n",
    "hate_labels = []\n",
    "\n",
    "\n",
    "# for idx, label in enumerate(bias_labels):\n",
    "#     train_df['bias_labels'] = [idx for b in train_df['bias'] if b ==label ]\n",
    "\n",
    "\n",
    "conditionlist = [\n",
    "    (train_df['bias'] == 'none') ,\n",
    "    (train_df['bias'] == 'others'),\n",
    "    (train_df['bias'] == 'gender')\n",
    "]\n",
    "choicelist = [0, 1, 2]\n",
    "\n",
    "train_df['bias_labels'] = np.select(conditionlist, choicelist, default='Not Specified')\n",
    "\n",
    "train_df['hate_labels'] = np.where(train_df['hate'] == 'none', 0, 1)\n",
    "\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c8d3600-60f1-45bb-bc01-94f92fad0d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8367 entries, 0 to 8366\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   title        8367 non-null   object\n",
      " 1   comment      8367 non-null   object\n",
      " 2   bias         8367 non-null   object\n",
      " 3   hate         8367 non-null   object\n",
      " 4   bias_labels  8367 non-null   object\n",
      " 5   hate_labels  8367 non-null   int64 \n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 392.3+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533b5e2d-cdc3-43b3-b34c-a8e30ad365da",
   "metadata": {},
   "source": [
    "# 4. Dataset 로드 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af064f2-9f46-4d57-9f17-5bf31beb77a3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19681b9e-d471-4b4a-8e44-7d502c4ed8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.json 에서 지정 이름별로 가져올 라이브러리 지정\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "TOKENIZER_CLASSES = {\n",
    "    \"BertTokenizer\": BertTokenizer,\n",
    "    \"AutoTokenizer\": AutoTokenizer,\n",
    "    \"ElectraTokenizer\": ElectraTokenizer,\n",
    "    \"AlbertTokenizer\": AlbertTokenizer,\n",
    "    \"AutoTokenizer\": AutoTokenizer\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed405c1d-7506-436d-9da8-91a0d22169d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='beomi/KcELECTRA-base', vocab_size=50135, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "TOKENIZER = TOKENIZER_CLASSES[args.tokenizer_class].from_pretrained(args.pretrained_model)\n",
    "if DEBUG==True:\n",
    "    print(TOKENIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18229b14-3fa0-4a75-887f-fdea2face10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "\n",
    "emojis = ''.join(emoji.UNICODE_EMOJI.keys())\n",
    "pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-ㅣ가-힣{emojis}]+')\n",
    "url_pattern = re.compile(\n",
    "    r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "\n",
    "def clean_text(x):\n",
    "    x = pattern.sub(' ', x)\n",
    "    x = url_pattern.sub('', x)\n",
    "    x = x.strip()\n",
    "    x = repeat_normalize(x, num_repeats=2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef02ae68-30b8-42b8-a214-4b20aa3a4119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len, label, mode='train', clean=None):\n",
    "        self.data = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.label = label\n",
    "        self.mode = mode\n",
    "        self.clean = clean\n",
    "        \n",
    "        if self.mode != 'test':\n",
    "            try:\n",
    "                if self.label == 'hate':\n",
    "                    self.labels = df['hate_labels'].tolist()\n",
    "                else:\n",
    "                    self.labels = df['bias_labels'].tolist()\n",
    "            except:\n",
    "                assert False, \"CustomDataset Error! : \\'label\\' columns does not exist in the dataframe. check mode is not train\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        title = self.data.title.iloc[idx]\n",
    "        comment = self.data.comment.iloc[idx]\n",
    "        \n",
    "        if self.clean is not None:\n",
    "            title = self.clean(title)\n",
    "            comment = self.clean(comment)\n",
    "        \n",
    "        \n",
    "        tokenized_text = self.tokenizer(title,\n",
    "                                       comment,\n",
    "                                       padding='max_length',\n",
    "                                       max_length=self.max_len,\n",
    "                                       truncation=True,\n",
    "                                       return_token_type_ids=True,\n",
    "                                       return_attention_mask=True,\n",
    "                                       return_tensors='pt')\n",
    "        data = {\n",
    "            'input_ids': tokenized_text['input_ids'].clone().detach().long(),\n",
    "            'attention_mask': tokenized_text['attention_mask'].clone().detach().long(),\n",
    "            'token_type_ids': tokenized_text['token_type_ids'].clone().detach().long()\n",
    "        }\n",
    "        \n",
    "        if self.mode != 'test':\n",
    "            if self.label == 'hate':\n",
    "                label = self.data.hate_labels.iloc[idx]\n",
    "            else:\n",
    "                label = self.data.bias_labels.iloc[idx]\n",
    "            \n",
    "            return data, label\n",
    "        else:\n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72da18d6-0fa6-40ce-b84b-438cf67c0b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset loaded.\n"
     ]
    }
   ],
   "source": [
    "train_hate_dataset = CustomDataset(df=train_df, tokenizer=TOKENIZER, max_len=args.max_seq_len, label='hate', mode='train', clean=clean_text)\n",
    "train_bias_dataset = CustomDataset(df=train_df, tokenizer=TOKENIZER, max_len=args.max_seq_len, label='bias', mode='train', clean=clean_text)\n",
    "print(\"train dataset loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18445b79-7870-40c8-899c-d5be228bc223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train bias dataset:  7530\n",
      "Train hate dataset:  7530\n",
      "Validation bias dataset:  837\n",
      "Validation hate dataset:  837\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "                                                         \n",
    "train_data, val_data = train_test_split(train_df, test_size=0.1, random_state=args.seed)\n",
    "\n",
    "train_bias_dataset = CustomDataset(train_data, TOKENIZER, args.max_seq_len, 'bias', 'train', clean=clean_text)\n",
    "train_hate_dataset = CustomDataset(train_data, TOKENIZER, args.max_seq_len, 'hate', 'train', clean=clean_text)\n",
    "\n",
    "val_bias_dataset = CustomDataset(val_data, TOKENIZER, args.max_seq_len, 'bias', 'validation', clean=clean_text)\n",
    "val_hate_dataset = CustomDataset(val_data, TOKENIZER, args.max_seq_len, 'hate', 'validation', clean=clean_text)\n",
    "\n",
    "print(\"Train bias dataset: \", len(train_bias_dataset))\n",
    "print(\"Train hate dataset: \", len(train_hate_dataset))\n",
    "\n",
    "print(\"Validation bias dataset: \", len(val_bias_dataset))\n",
    "print(\"Validation hate dataset: \", len(val_hate_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15144f1d-3375-47a8-be14-4bb640b72848",
   "metadata": {},
   "source": [
    "# 5. 분류 모델 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0afa39b8-5131-4964-8cd5-f0aaa70517a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "BASE_MODEL = {\n",
    "    \"BertForSequenceClassification\": BertForSequenceClassification,\n",
    "    \"AutoModel\": AutoModel,\n",
    "    \"ElectraForSequenceClassification\": ElectraForSequenceClassification,\n",
    "    \"AlbertForSequenceClassification\": AlbertForSequenceClassification,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6d65ea1-d140-4b86-9272-518d19038547",
   "metadata": {},
   "outputs": [],
   "source": [
    "biasModel = BASE_MODEL[args.architecture].from_pretrained(args.pretrained_model,\n",
    "                                                       num_labels = args.bias_num_classes,\n",
    "                                                       output_attentions=False, \n",
    "                                                       output_hidden_states=True)\n",
    "\n",
    "hateModel = BASE_MODEL[args.architecture].from_pretrained(args.pretrained_hate_model,\n",
    "                                                       num_labels = args.hate_num_classes,\n",
    "                                                       output_attentions=False, \n",
    "                                                       output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e236ade-275e-40a6-b3ea-de5ca628d4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### v2 에서 일부 수정됨\n",
    "class myClassifier(nn.Module):\n",
    "    def __init__(self, model, num_classes, hidden_size = 768, selected_layers=False, params=None):\n",
    "        super(myClassifier, self).__init__()\n",
    "        self.model = model\n",
    "        self.softmax = nn.Softmax(dim=1) \n",
    "        self.selected_layers = selected_layers\n",
    "        self.num_classes = num_classes\n",
    "        # 사실 dr rate은 model config 에서 hidden_dropout_prob로 가져와야 하는데 bert에선 0.1이 쓰였음\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "\n",
    "    def forward(self, token_ids, attention_mask, segment_ids):      \n",
    "        outputs = self.model(input_ids = token_ids, \n",
    "                             token_type_ids = segment_ids.long(), \n",
    "                             attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        \n",
    "        # hidden state에서 마지막 4개 레이어를 뽑아 합쳐 새로운 pooled output 을 만드는 시도\n",
    "        if self.selected_layers == True:\n",
    "            hidden_states = outputs.hidden_states\n",
    "            pooled_output = torch.cat(tuple([hidden_states[i] for i in [-4, -3, -2, -1]]), dim=-1)\n",
    "            # print(\"concatenated output shape: \", pooled_output.shape)\n",
    "            ## dim(batch_size, max_seq_len, hidden_dim) 에서 가운데를 0이라 지정함으로, [cls] 토큰의 임베딩을 가져온다. \n",
    "            ## (text classification 구조 참고)\n",
    "            pooled_output = pooled_output[:, 0, :]\n",
    "            # print(pooled_output)\n",
    "\n",
    "            pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "            ## 3개의 레이어를 합치므로 classifier의 차원은 (hidden_dim, 6)이다\n",
    "            classifier = nn.Linear(pooled_output.shape[1], self.num_classes).to(token_ids.device)\n",
    "            logits = classifier(pooled_output)\n",
    "        \n",
    "        else:\n",
    "            logits=outputs.logits\n",
    "        \n",
    "    \n",
    "        # 각 클래스별 확률\n",
    "        prob= self.softmax(logits)\n",
    "        # print(prob)\n",
    "        # logits2 = outputs.logits\n",
    "        # print(self.softmax(logits2))\n",
    "\n",
    "\n",
    "        return logits, prob\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.num_classes)\n",
    "        \n",
    "# 마지막 4 hidden layers concat 하는 방법을 쓰신다면 True로 변경        \n",
    "bias_model = myClassifier(biasModel, num_classes=args.bias_num_classes, selected_layers=False)\n",
    "hate_model = myClassifier(hateModel, num_classes=args.hate_num_classes, selected_layers=False)\n",
    "\n",
    "# if DEBUG ==True :\n",
    "#     print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6666ac9c-d2ba-4f3c-b6db-dfa50c3a6271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{bias_model}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81c07511-1b33-4bc1-9b8c-3d1d031d109b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "model.electra.embeddings.word_embeddings.weight         (50135, 768)\n",
      "model.electra.embeddings.position_embeddings.weight       (512, 768)\n",
      "model.electra.embeddings.token_type_embeddings.weight       (2, 768)\n",
      "model.electra.embeddings.LayerNorm.weight                     (768,)\n",
      "model.electra.embeddings.LayerNorm.bias                       (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "model.electra.encoder.layer.0.attention.self.query.weight   (768, 768)\n",
      "model.electra.encoder.layer.0.attention.self.query.bias       (768,)\n",
      "model.electra.encoder.layer.0.attention.self.key.weight   (768, 768)\n",
      "model.electra.encoder.layer.0.attention.self.key.bias         (768,)\n",
      "model.electra.encoder.layer.0.attention.self.value.weight   (768, 768)\n",
      "model.electra.encoder.layer.0.attention.self.value.bias       (768,)\n",
      "model.electra.encoder.layer.0.attention.output.dense.weight   (768, 768)\n",
      "model.electra.encoder.layer.0.attention.output.dense.bias       (768,)\n",
      "model.electra.encoder.layer.0.attention.output.LayerNorm.weight       (768,)\n",
      "model.electra.encoder.layer.0.attention.output.LayerNorm.bias       (768,)\n",
      "model.electra.encoder.layer.0.intermediate.dense.weight  (3072, 768)\n",
      "model.electra.encoder.layer.0.intermediate.dense.bias        (3072,)\n",
      "model.electra.encoder.layer.0.output.dense.weight        (768, 3072)\n",
      "model.electra.encoder.layer.0.output.dense.bias               (768,)\n",
      "model.electra.encoder.layer.0.output.LayerNorm.weight         (768,)\n",
      "model.electra.encoder.layer.0.output.LayerNorm.bias           (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "model.classifier.dense.weight                             (768, 768)\n",
      "model.classifier.dense.bias                                   (768,)\n",
      "model.classifier.out_proj.weight                            (3, 768)\n",
      "model.classifier.out_proj.bias                                  (3,)\n"
     ]
    }
   ],
   "source": [
    "if DEBUG==True:\n",
    "    params = list(bias_model.named_parameters())\n",
    "\n",
    "    print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "    print('==== Embedding Layer ====\\n')\n",
    "\n",
    "    for p in params[0:5]:\n",
    "        print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "    print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "    for p in params[5:21]:\n",
    "        print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "    print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "    for p in params[-4:]:\n",
    "        print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731809b2-cb92-4294-ab20-bc6f86785af2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6. 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3200bc-284d-46c2-b1f5-ea345a5bdb35",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6-1. Early Stopper 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50ffe8ed-3093-4319-ae45-f1bbcee458ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossEarlyStopper():\n",
    "    \"\"\"Early stopper\n",
    "\n",
    "        patience (int): loss가 줄어들지 않아도 학습할 epoch 수\n",
    "        patience_counter (int): loss 가 줄어들지 않을 때 마다 1씩 증가\n",
    "        min_loss (float): 최소 loss\n",
    "        stop (bool): True 일 때 학습 중단\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience: int)-> None:\n",
    "        \"\"\" 초기화\n",
    "\n",
    "        Args:\n",
    "            patience (int): loss가 줄어들지 않아도 학습할 epoch 수\n",
    "            weight_path (str): weight 저장경로\n",
    "            verbose (bool): 로그 출력 여부, True 일 때 로그 출력\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.patience_counter = 0\n",
    "        self.min_loss = np.Inf\n",
    "        self.stop = False\n",
    "\n",
    "    def check_early_stopping(self, loss: float)-> None:\n",
    "        # 첫 에폭\n",
    "        if self.min_loss == np.Inf:\n",
    "            self.min_loss = loss\n",
    "           \n",
    "        # loss가 줄지 않는다면 -> patience_counter 1 증가\n",
    "        elif loss > self.min_loss:\n",
    "            self.patience_counter += 1\n",
    "            msg = f\"Early stopping counter {self.patience_counter}/{self.patience}\"\n",
    "\n",
    "            # patience 만큼 loss가 줄지 않았다면 학습을 중단합니다.\n",
    "            if self.patience_counter == self.patience:\n",
    "                self.stop = True\n",
    "            print(msg)\n",
    "        # loss가 줄어듬 -> min_loss 갱신, patience_counter 초기화\n",
    "        elif loss <= self.min_loss:\n",
    "            self.patience_counter = 0\n",
    "            ### v2 에서 수정됨\n",
    "            ### self.save_model = True -> 삭제 (사용하지 않음)\n",
    "            msg = f\"Validation loss decreased {self.min_loss} -> {loss}\"\n",
    "            self.min_loss = loss\n",
    "\n",
    "            print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181877a2-c419-4126-9b80-575801908c7d",
   "metadata": {},
   "source": [
    "## 6-2. Epoch 별 학습 및 검증"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f942a3e9-3ac5-49e4-b282-6526935001eb",
   "metadata": {},
   "source": [
    "- [Transformers optimization documentation](https://huggingface.co/docs/transformers/main_classes/optimizer_schedules)\n",
    "- [스케줄러 documentation](https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#schedules)\n",
    "- Adam optimizer의 epsilon 파라미터 eps = 1e-8 는 \"계산 중 0으로 나눔을 방지 하기 위한 아주 작은 숫자 \" 입니다. ([출처](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/))\n",
    "- 스케줄러 파라미터\n",
    "    - `warmup_ratio` : \n",
    "      - 학습이 진행되면서 학습률을 그 상황에 맞게 가변적으로 적당하게 변경되게 하기 위해 Scheduler를 사용합니다.\n",
    "      - 처음 학습률(Learning rate)를 warm up하기 위한 비율을 설정하는 warmup_ratio을 설정합니다.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c66874a-66ca-4787-bb8a-c02f0fedb9cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 0/471 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config file loaded\n",
      "bias :  monologg/koelectra-base-v3-bias\n",
      "hate :  beomi/beep-KcELECTRA-base-hate\n",
      "RUN :  demo3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5506/2873126547.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_bias_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_bias_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_num_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhate_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_hate_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_hate_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5506/2873126547.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data, val_data, args, num_classes, mode)\u001b[0m\n\u001b[1;32m     63\u001b[0m                     \u001b[0;31m# print(type(train_label))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                     \u001b[0;31m# print(train_label)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                     \u001b[0mtrain_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "args = set_config(config_path)\n",
    "\n",
    "logging.set_verbosity_warning()\n",
    "\n",
    "# 재현을 위해 모든 곳의 시드 고정\n",
    "seed_val = args.seed\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "def train(model, train_data, val_data, args, num_classes, mode = 'train'):\n",
    "    \n",
    "    # args.run은 실험 이름 (어디까지나 팀원들간의 버전 관리 및 공유 편의를 위한 것으로, 자유롭게 수정 가능합니다.)\n",
    "    print(\"RUN : \", args.run)\n",
    "    shutil.copyfile(\"config/config.json\", os.path.join(args.config_dir, f\"config_{args.run}.json\"))\n",
    "\n",
    "    early_stopper = LossEarlyStopper(patience=args.patience)\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=args.train_batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_data, batch_size=args.train_batch_size)\n",
    "\n",
    "    \n",
    "    # if DEBUG == True:\n",
    "    #     # 데이터로더가 성공적으로 로드 되었는지 확인\n",
    "    #     for idx, data in enumerate(train_dataloader):\n",
    "    #         if idx==0:\n",
    "    #             print(\"batch size : \", len(data[0]['input_ids']))\n",
    "    #             print(\"The first batch looks like ..\\n\", data[0])\n",
    "    \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    total_steps = len(train_dataloader) * args.train_epochs\n",
    "\n",
    "    ### v2에서 수정됨 (Adam -> AdamW)\n",
    "    optimizer = AdamW(model.parameters(), lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(total_steps * args.warmup_proportion), \n",
    "                                                num_training_steps=total_steps)\n",
    "\n",
    "    \n",
    "    if use_cuda:\n",
    "        model = model.to(DEVICE)\n",
    "        criterion = criterion.to(DEVICE)\n",
    "        \n",
    "\n",
    "    tr_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    best_score = 0.0\n",
    "    best_loss = np.inf\n",
    "      \n",
    "\n",
    "    for epoch_num in range(args.train_epochs):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "            \n",
    "            assert mode in ['train', 'val'], 'your mode should be either \\'train\\' or \\'val\\''\n",
    "            \n",
    "            if mode =='train':\n",
    "                for train_input, train_label in tqdm(train_dataloader):\n",
    "                    # print(train_input, train_label)\n",
    "                    # print(type(train_label))\n",
    "                    # print(train_label)\n",
    "                    train_label = torch.tensor(train_label)\n",
    "                    print(train_label, type(train_label))\n",
    "                    \n",
    "                    mask = train_input['attention_mask'].to(DEVICE)\n",
    "                    input_id = train_input['input_ids'].squeeze(1).to(DEVICE)\n",
    "                    segment_ids = train_input['token_type_ids'].squeeze(1).to(DEVICE)\n",
    "                    train_label = train_label.long().to(DEVICE)  \n",
    "                    \n",
    "                    ### v2에 수정됨\n",
    "                    optimizer.zero_grad()\n",
    " \n",
    "                    output = model(input_id, mask, segment_ids)\n",
    "                    batch_loss = criterion(output[0].view(-1, num_classes), train_label.view(-1))\n",
    "                    total_loss_train += batch_loss.item()\n",
    "\n",
    "                    acc = (output[0].argmax(dim=1) == train_label).sum().item()\n",
    "                    total_acc_train += acc\n",
    "                    \n",
    "                    ### v2에 수정됨\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    batch_loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    ### v2 에 수정됨\n",
    "                    scheduler.step()\n",
    "                    \n",
    "\n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "            \n",
    "            # validation을 위해 이걸 넣으면 이 evaluation 프로세스 중엔 dropout 레이어가 다르가 동작한다.\n",
    "            model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "                    val_label = torch.stack(list(val_label), dim=0)\n",
    "                    \n",
    "                    mask = val_input['attention_mask'].to(DEVICE)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(DEVICE)\n",
    "                    segment_ids = val_input['token_type_ids'].squeeze(1).to(DEVICE)\n",
    "                    val_label = val_label.long().to(DEVICE)\n",
    "\n",
    "                    output = model(input_id, mask, segment_ids)\n",
    "                    ### v2 에서 일부 수정 (output -> output[0]로 myClassifier 모델에 정의된대로 logits 가져옴)\n",
    "                    batch_loss = criterion(output[0].view(-1, num_classes), val_label.view(-1))\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    ### v2 에서 일부 수정 (output -> output[0]로 myClassifier 모델에 정의된대로 logits 가져옴)\n",
    "                    acc = (output[0].argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            \n",
    "            train_loss = total_loss_train / len(train_data)\n",
    "            train_accuracy = total_acc_train / len(train_data)\n",
    "            val_loss = total_loss_val / len(val_data)\n",
    "            val_accuracy = total_acc_val / len(val_data)\n",
    "            \n",
    "            # 한 Epoch 학습 후 학습/검증에 대해 loss와 평가지표 (여기서는 accuracy로 임의로 설정) 출력\n",
    "            print(\n",
    "                f'Epoch: {epoch_num + 1} \\\n",
    "                | Train Loss: {train_loss: .3f} \\\n",
    "                | Train Accuracy: {train_accuracy: .3f} \\\n",
    "                | Val Loss: {val_loss: .3f} \\\n",
    "                | Val Accuracy: {val_accuracy: .3f}')\n",
    "          \n",
    "            # early_stopping check\n",
    "            early_stopper.check_early_stopping(loss=val_loss)\n",
    "\n",
    "            if early_stopper.stop:\n",
    "                print('Early stopped, Best score : ', best_score)\n",
    "                break\n",
    "\n",
    "            ### v2 에 수정됨\n",
    "            ### loss와 accuracy가 꼭 correlate하진 않습니다.\n",
    "            ### \n",
    "            ### 원본 (필요하다면 다시 해제 후 사용)\n",
    "            # if val_accuracy > best_score : \n",
    "            if val_loss < best_loss :\n",
    "            # 모델이 개선됨 -> 검증 점수와 베스트 loss, weight 갱신\n",
    "                best_score = val_accuracy \n",
    "                \n",
    "                ### v2에서 추가\n",
    "                best_loss =val_loss\n",
    "                # 학습된 모델을 저장할 디렉토리 및 모델 이름 지정\n",
    "                SAVED_MODEL =  os.path.join(args.result_dir, f'{model}_best_{args.run}.pt')\n",
    "            \n",
    "                check_point = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'scheduler': scheduler.state_dict()\n",
    "                }\n",
    "                torch.save(check_point, SAVED_MODEL)  \n",
    "              \n",
    "            # print(\"scheduler : \", scheduler.state_dict())\n",
    "\n",
    "\n",
    "    print(\"train finished\")\n",
    "\n",
    "\n",
    "train(bias_model, train_bias_dataset, val_bias_dataset, args, args.bias_num_classes, mode = 'train')\n",
    "train(hate_model, train_hate_dataset, val_hate_dataset, args, 2, mode = 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e54290-7e37-48db-9adc-814c65722520",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 7. Test dataset으로 추론 (Prediction)\n",
    "\n",
    "\n",
    "- v2 에서 수정된 부분\n",
    "    - output -> output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b10fc4-a5e0-4f62-bc5c-e862b813674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 테스트 데이터셋 불러오기\n",
    "test_data = CustomDataset(test_df, tokenizer = TOKENIZER, max_len= args.max_seq_len, mode='test')\n",
    "\n",
    "def test(model, SAVED_MODEL, test_data, args, mode = 'test'):\n",
    "\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=args.eval_batch_size)\n",
    "\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.to(DEVICE)\n",
    "        model.load_state_dict(torch.load(SAVED_MODEL)['model'])\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_input in test_dataloader:\n",
    "\n",
    "            mask = test_input['attention_mask'].to(DEVICE)\n",
    "            input_id = test_input['input_ids'].squeeze(1).to(DEVICE)\n",
    "            segment_ids = test_input['token_type_ids'].squeeze(1).to(DEVICE)\n",
    "\n",
    "            output = model(input_id, mask, segment_ids)\n",
    "\n",
    "            output = output[0].argmax(dim=1).cpu().tolist()\n",
    "\n",
    "            for label in output:\n",
    "                pred.append(label)\n",
    "                \n",
    "    return pred\n",
    "\n",
    "SAVED_MODEL =  os.path.join(args.result_dir, f'best_{args.run}.pt')\n",
    "\n",
    "bias_pred = test(bias_model, SAVED_MODEL, test_data, args)\n",
    "hate_pred = test(hate_model, SAVED_MODEL, test_data, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aebb740-ae18-43c3-88cc-4aff7d8f62e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"prediction completed for \", len(pred), \"comments\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319aa814-d69e-456a-9a04-71389382668f",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceb7da8-e936-4933-a440-8e0678777ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-5 사이의 라벨 값 별로 bias, hate로 디코딩 하기 위한 딕셔너리\n",
    "bias_dict = {0: 'none', 1: 'none', 2: 'others', 3:'others', 4:'gender', 5:'gender'}\n",
    "hate_dict = {0: 'none', 1: 'hate', 2: 'none', 3:'hate', 4:'none', 5:'hate'}\n",
    "\n",
    "# 인코딩 값으로 나온 타겟 변수를 디코딩\n",
    "pred_bias = ['' for i in range(len(pred))]\n",
    "pred_hate = ['' for i in range(len(pred))]\n",
    "\n",
    "for idx, label in enumerate(pred):\n",
    "    pred_bias[idx]=(str(bias_dict[label]))\n",
    "    pred_hate[idx]=(str(hate_dict[label]))\n",
    "print('decode Completed!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74d84ac-2aff-4028-86ec-7b52c24113cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv(os.path.join(args.data_dir,'sample_submission.csv'))\n",
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698dc2f5-cb20-497f-a7b7-c50fc7d9ec16",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit['bias'] = pred_bias\n",
    "submit['hate'] = pred_hate\n",
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d3041a-e0d2-4666-8a97-055596c35971",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv(os.path.join(args.result_dir, f\"submission_{args.run}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4305bd42-3aaa-42ab-acad-ca710d70d1c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70e53a9-2362-434c-ba59-b9567e2a3d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cb1865-79ed-44f9-962b-b56f28a82f30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f101177f-c4ad-48dd-9794-1d43a7687657",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
