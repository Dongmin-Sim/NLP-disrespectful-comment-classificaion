{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cba9f994-43dd-4df3-ae07-d1634d309849",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 비매너 댓글 식별"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb7f13d-e589-4b85-bfbf-042bf0683622",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. 라이브러리 로드 & 환경설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e260163-8005-4c13-be21-a31b0f400c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "from attrdict import AttrDict\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import *\n",
    "from torch.optim import Adam, AdamW\n",
    "\n",
    "from transformers import logging, get_linear_schedule_with_warmup\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizer,  \n",
    "    AutoTokenizer,\n",
    "    ElectraTokenizer,\n",
    "    AlbertTokenizer\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    BertModel,\n",
    "    AutoModel, \n",
    "    ElectraForSequenceClassification,\n",
    "    BertForSequenceClassification,\n",
    "    AlbertForSequenceClassification\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd853af7-0340-4466-97ba-abf7753843d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1-1 DEVICE 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "476bd575-ac51-49ce-94f7-0bc80db5b040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of GPUs:  1\n",
      "Does GPU exist? :  True\n"
     ]
    }
   ],
   "source": [
    "# 사용할 GPU 지정\n",
    "print(\"number of GPUs: \", torch.cuda.device_count())\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Does GPU exist? : \", use_cuda)\n",
    "DEVICE = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03da9dc5-b84a-460c-b840-4c1d61df350a",
   "metadata": {},
   "source": [
    "### 1-2 DEBUG 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "131727e7-63d0-43e2-9b6b-2b00b6eb046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True 일 때 코드를 실행하면 example 등을 보여줌\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e6891c-90d1-4f1b-aecb-08feecdfc1f0",
   "metadata": {},
   "source": [
    "### Config 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d8e1859-2154-4207-8f53-852ddad5ed4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config/config.json\n",
      "config file loaded\n",
      "beomi/KcELECTRA-base\n"
     ]
    }
   ],
   "source": [
    "config_path = os.path.join('config', 'config.json')\n",
    "print(config_path)\n",
    "\n",
    "# config 적용 \n",
    "def set_config(config_path):\n",
    "    if os.path.lexists(config_path):\n",
    "        with open(config_path, 'r') as f:\n",
    "            args = AttrDict(json.load(f))\n",
    "            \n",
    "            print('config file loaded')\n",
    "            print(args.pretrained_model)\n",
    "    else:\n",
    "        assert False, 'config json file cannot be found.. please check the path again.'\n",
    "        \n",
    "    return args\n",
    "\n",
    "args = set_config(config_path)\n",
    "\n",
    "os.makedirs(args.result_dir, exist_ok=True)\n",
    "os.makedirs(args.config_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaa7786-2a53-46a1-aaf0-67da9d84fe8a",
   "metadata": {},
   "source": [
    "# 2. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3832d0-36c0-4286-aa6e-2a4ef4fc0595",
   "metadata": {},
   "source": [
    "## 2-1. Train data 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8d65d89-beab-43c9-9a7d-bf914c219e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(args.data_dir, 'train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e422d745-6808-4b3b-812e-f4a1ef7d7bd5",
   "metadata": {},
   "source": [
    "## 2-2 Train data 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb1fb4bd-555d-4502-96a4-9f329ad033b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>comment</th>\n",
       "      <th>bias</th>\n",
       "      <th>hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"'미스터 션샤인' 변요한, 김태리와 같은 양복 입고 학당 방문! 이유는?\"</td>\n",
       "      <td>김태리 정말 연기잘해 진짜</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"[SC현장]\"\"극사실주의 현실♥\"\"…'가장 보통의 연애' 김래원X공효진, 16년만...</td>\n",
       "      <td>공효진 발연기나이질생각이읍던데 왜계속주연일까</td>\n",
       "      <td>none</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"손연재, 리듬체조 학원 선생님 \"\"하고 싶은 일 해서 행복하다\"\"\"</td>\n",
       "      <td>누구처럼 돈만 밝히는 저급인생은 살아가지마시길~~ 행복은 머니순이 아니니깐 작은거에...</td>\n",
       "      <td>others</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"'섹션TV' 김해숙 \"\"'허스토리' 촬영 후 우울증 얻었다\"\"\"</td>\n",
       "      <td>일본 축구 져라</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"[단독] 임현주 아나운서 “‘노브라 챌린지’ 방송 덕에 낸 용기, 자연스런 논의의...</td>\n",
       "      <td>난 절대로 임현주 욕하는인간이랑은 안논다 @.@</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0         \"'미스터 션샤인' 변요한, 김태리와 같은 양복 입고 학당 방문! 이유는?\"   \n",
       "1  \"[SC현장]\"\"극사실주의 현실♥\"\"…'가장 보통의 연애' 김래원X공효진, 16년만...   \n",
       "2             \"손연재, 리듬체조 학원 선생님 \"\"하고 싶은 일 해서 행복하다\"\"\"   \n",
       "3               \"'섹션TV' 김해숙 \"\"'허스토리' 촬영 후 우울증 얻었다\"\"\"   \n",
       "4  \"[단독] 임현주 아나운서 “‘노브라 챌린지’ 방송 덕에 낸 용기, 자연스런 논의의...   \n",
       "\n",
       "                                             comment    bias  hate  \n",
       "0                                     김태리 정말 연기잘해 진짜    none  none  \n",
       "1                           공효진 발연기나이질생각이읍던데 왜계속주연일까    none  hate  \n",
       "2  누구처럼 돈만 밝히는 저급인생은 살아가지마시길~~ 행복은 머니순이 아니니깐 작은거에...  others  hate  \n",
       "3                                           일본 축구 져라    none  none  \n",
       "4                         난 절대로 임현주 욕하는인간이랑은 안논다 @.@    none  none  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(train_path, encoding='UTF-8-SIG')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22bcbe44-0792-4f0a-b9fa-eb352c919fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data 개수 :  8367\n"
     ]
    }
   ],
   "source": [
    "print(\"train data 개수 : \", len(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedd832e-9026-4785-bbee-90c93b646d2e",
   "metadata": {},
   "source": [
    "## 2-3 Train 데이터 분포\n",
    "\n",
    "title의 길이 분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9a3b82b-895c-4dfc-8951-ed34798be285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = train_df['title'].str.len().hist()\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acce4696-9e7d-43e8-b2b2-fb05e7437ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title 최대 길이 :  63\n",
      "title 평균 길이 :  42.4844030118322\n"
     ]
    }
   ],
   "source": [
    "print('title 최대 길이 : ', train_df['title'].str.len().max())\n",
    "print('title 평균 길이 : ', train_df['title'].str.len().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16f7a8ad-8282-4192-b2b9-129aa79bd56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = train_df['comment'].str.len().hist()\n",
    "\n",
    "print()\n",
    "\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a515035-f00c-4b77-92b1-c2614e7ffe37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comment 최대 길이 :  137\n",
      "comment 평균 길이 :  38.72439345046014\n"
     ]
    }
   ],
   "source": [
    "print('comment 최대 길이 : ', train_df['comment'].str.len().max())\n",
    "print('comment 평균 길이 : ', train_df['comment'].str.len().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a63bed4-b58e-4ccf-9d01-840d7898ddc5",
   "metadata": {},
   "source": [
    "### bias와 hate 비중"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2253f64-f506-4ce5-9239-cc71b652829c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias 클래스 :  ['none' 'others' 'gender']\n",
      "hate 클래스 :  ['none' 'hate']\n"
     ]
    }
   ],
   "source": [
    "print('bias 클래스 : ', train_df.bias.unique())\n",
    "print('hate 클래스 : ', train_df.hate.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e40882-7172-4fc1-bf70-931d750f125f",
   "metadata": {},
   "source": [
    "**bias 수**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7216be8d-36d2-4749-9fe2-f765efb77ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias-none 개수 : 5490 / 비율 : 65.61%\n",
      "bias-others 개수 : 1578 / 비율 : 18.86%\n",
      "bias-gender 개수 : 1299 / 비율 : 15.53%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "sns.histplot(x=train_df['bias'])\n",
    "\n",
    "count_none = sum(train_df.bias == 'none')\n",
    "count_others = sum(train_df.bias == 'others')\n",
    "count_gender = sum(train_df.bias == 'gender')\n",
    "\n",
    "print(f'bias-none 개수 : {count_none} / 비율 : {round(count_none/len(train_df)*100, 2)}%')\n",
    "print(f'bias-others 개수 : {count_others} / 비율 : {round(count_others/len(train_df)*100, 2)}%')\n",
    "print(f'bias-gender 개수 : {count_gender} / 비율 : {round(count_gender/len(train_df)*100, 2)}%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99eecd5-d747-4f46-a76a-d48a8ae32cba",
   "metadata": {},
   "source": [
    "**hate 수**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4da99f76-978c-489c-b581-fd0ed28e6c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hate-none 개수 : 3646 / 비율 : 43.58%\n",
      "hate-hate 개수 : 4721 / 비율 : 56.42%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "sns.histplot(x=train_df['hate'])\n",
    "\n",
    "count_hate_none = sum(train_df.hate == 'none')\n",
    "count_hate_hate = sum(train_df.hate == 'hate')\n",
    "\n",
    "print(f'hate-none 개수 : {count_hate_none} / 비율 : {round(count_hate_none/len(train_df)*100, 2)}%')\n",
    "print(f'hate-hate 개수 : {count_hate_hate} / 비율 : {round(count_hate_hate/len(train_df)*100, 2)}%')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca07155-ed80-49ba-a9ed-f0c096950d5c",
   "metadata": {},
   "source": [
    "**bias, hate 비율**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74338329-dcce-4402-803d-8dc8771b10a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8367"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "870b766f-406d-4648-8e33-e8e0589100bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">comment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hate</th>\n",
       "      <th>hate</th>\n",
       "      <th>none</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bias</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>1216</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>none</th>\n",
       "      <td>2068</td>\n",
       "      <td>3422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>others</th>\n",
       "      <td>1437</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       comment      \n",
       "hate      hate  none\n",
       "bias                \n",
       "gender    1216    83\n",
       "none      2068  3422\n",
       "others    1437   141"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_hate = train_df.iloc[:, 1:].pivot_table(index='bias', columns='hate', aggfunc='count')\n",
    "bias_hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "174930a1-9e2c-417a-bd24-1bc289129702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'bias-hate heat map')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "sns.heatmap(bias_hate, cmap=sns.light_palette('blue', as_cmap=True), annot=True, fmt='g')\n",
    "plt.title(\"bias-hate heat map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1120c740-2658-4441-b7c0-bd7841fd798c",
   "metadata": {},
   "source": [
    "bias와 hate의 관계성? 연관성이 존재하는가?? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86d950e-3761-4636-a457-41aa6844362b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "410095ce-b6b5-4426-abb8-bf768c5192c6",
   "metadata": {},
   "source": [
    "## 2-5. Test 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df37cdb6-9374-4032-a638-6f839abafc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 데이터 경로가 올바른가요? :  True\n"
     ]
    }
   ],
   "source": [
    "test_path = os.path.join(args.data_dir,'test.csv')\n",
    "print(\"test 데이터 경로가 올바른가요? : \", os.path.lexists(test_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9521ae07-7c62-4882-9ee0-e1c7393b3f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>류현경♥︎박성훈, 공개연애 4년차 애정전선 이상無..\"의지 많이 된다\"[종합]</td>\n",
       "      <td>둘다 넘 좋다~행복하세요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\"현금 유도+1인 1라면?\"…'골목식당' 백종원, 초심 잃은 도시락집에 '경악' [종합]</td>\n",
       "      <td>근데 만원이하는 현금결제만 하라고 써놓은집 우리나라에 엄청 많은데</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>입대 D-11' 서은광의 슬픈 멜로디..비투비, 눈물의 첫 체조경기장[콘서트 종합]</td>\n",
       "      <td>누군데 얘네?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>아이콘택트' 리쌍 길, 3년 전 결혼설 부인한 이유 공개…\"결혼,출산 숨겼다\"</td>\n",
       "      <td>쑈 하지마라 짜식아!음주 1번은 실수, 2번은 고의, 3번은 인간쓰레기다.슬금슬금 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>구하라, 안검하수 반박 해프닝...\"당당하다\"vs\"그렇게까지\" 설전 [종합]</td>\n",
       "      <td>안검하수 가지고 있는 분께 희망을 주고 싶은건가요? 수술하면 이렇게 자연스러워진다고...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                              title  \\\n",
       "0   0        류현경♥︎박성훈, 공개연애 4년차 애정전선 이상無..\"의지 많이 된다\"[종합]   \n",
       "1   1  \"현금 유도+1인 1라면?\"…'골목식당' 백종원, 초심 잃은 도시락집에 '경악' [종합]   \n",
       "2   2     입대 D-11' 서은광의 슬픈 멜로디..비투비, 눈물의 첫 체조경기장[콘서트 종합]   \n",
       "3   3        아이콘택트' 리쌍 길, 3년 전 결혼설 부인한 이유 공개…\"결혼,출산 숨겼다\"   \n",
       "4   4         구하라, 안검하수 반박 해프닝...\"당당하다\"vs\"그렇게까지\" 설전 [종합]   \n",
       "\n",
       "                                             comment  \n",
       "0                                      둘다 넘 좋다~행복하세요  \n",
       "1               근데 만원이하는 현금결제만 하라고 써놓은집 우리나라에 엄청 많은데  \n",
       "2                                            누군데 얘네?  \n",
       "3  쑈 하지마라 짜식아!음주 1번은 실수, 2번은 고의, 3번은 인간쓰레기다.슬금슬금 ...  \n",
       "4  안검하수 가지고 있는 분께 희망을 주고 싶은건가요? 수술하면 이렇게 자연스러워진다고...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_path)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf1397d3-3449-4de3-9484-ecd9e70623df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = test_df['title'].str.len().hist()\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ddf9255-2037-454b-85c4-66a646bcc7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title 최대 길이 :  59\n",
      "title 평균 길이 :  38.86888454011741\n"
     ]
    }
   ],
   "source": [
    "print('title 최대 길이 : ', test_df['title'].str.len().max())\n",
    "print('title 평균 길이 : ', test_df['title'].str.len().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45b21c19-e232-47a5-971d-fbc1f2df0ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = test_df['comment'].str.len().hist()\n",
    "\n",
    "print()\n",
    "\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03642fe2-1252-47d7-b5ea-3d6cdfe099d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comment 최대 길이 :  123\n",
      "comment 평균 길이 :  36.31311154598826\n"
     ]
    }
   ],
   "source": [
    "print('comment 최대 길이 : ', test_df['comment'].str.len().max())\n",
    "print('comment 평균 길이 : ', test_df['comment'].str.len().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84a7de6-7a87-40ad-ad66-69fd927b456e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d78c6996-490e-4dbd-bc08-a94105ee7d20",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a7b6e2f-0b55-4ab7-aee6-33e0dd55f213",
   "metadata": {},
   "source": [
    "# 3. 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2585bcf7-1399-4de7-ac8d-54a1dbadd257",
   "metadata": {},
   "source": [
    "## 3-1. 라벨값 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d94fda10-d2dc-4d5b-8af7-9a8c1891e0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['none' 'none']\n",
      " ['none' 'hate']\n",
      " ['others' 'none']\n",
      " ['others' 'hate']\n",
      " ['gender' 'none']\n",
      " ['gender' 'hate']]\n"
     ]
    }
   ],
   "source": [
    "# 두 라벨의 가능한 모든 조합 만들기\n",
    "combinations = np.array(np.meshgrid(train_df.bias.unique(), train_df.hate.unique())).T.reshape(-1,2)\n",
    "\n",
    "if DEBUG==True:\n",
    "    print(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c65c5584-953f-4dc4-9ab8-ab36563d8655",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['none', 'none'], dtype=object), array(['none', 'hate'], dtype=object), array(['others', 'hate'], dtype=object), array(['none', 'none'], dtype=object), array(['none', 'none'], dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "# bias, hate 컬럼을 합친 것\n",
    "bias_hate = list(np.array([train_df['bias'].values, train_df['hate'].values]).T.reshape(-1,2))\n",
    "\n",
    "if DEBUG==True:\n",
    "    print(bias_hate[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5461f090-be97-4dbf-bb22-306fc9c2cab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>comment</th>\n",
       "      <th>bias</th>\n",
       "      <th>hate</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"'미스터 션샤인' 변요한, 김태리와 같은 양복 입고 학당 방문! 이유는?\"</td>\n",
       "      <td>김태리 정말 연기잘해 진짜</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"[SC현장]\"\"극사실주의 현실♥\"\"…'가장 보통의 연애' 김래원X공효진, 16년만...</td>\n",
       "      <td>공효진 발연기나이질생각이읍던데 왜계속주연일까</td>\n",
       "      <td>none</td>\n",
       "      <td>hate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"손연재, 리듬체조 학원 선생님 \"\"하고 싶은 일 해서 행복하다\"\"\"</td>\n",
       "      <td>누구처럼 돈만 밝히는 저급인생은 살아가지마시길~~ 행복은 머니순이 아니니깐 작은거에...</td>\n",
       "      <td>others</td>\n",
       "      <td>hate</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"'섹션TV' 김해숙 \"\"'허스토리' 촬영 후 우울증 얻었다\"\"\"</td>\n",
       "      <td>일본 축구 져라</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"[단독] 임현주 아나운서 “‘노브라 챌린지’ 방송 덕에 낸 용기, 자연스런 논의의...</td>\n",
       "      <td>난 절대로 임현주 욕하는인간이랑은 안논다 @.@</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0         \"'미스터 션샤인' 변요한, 김태리와 같은 양복 입고 학당 방문! 이유는?\"   \n",
       "1  \"[SC현장]\"\"극사실주의 현실♥\"\"…'가장 보통의 연애' 김래원X공효진, 16년만...   \n",
       "2             \"손연재, 리듬체조 학원 선생님 \"\"하고 싶은 일 해서 행복하다\"\"\"   \n",
       "3               \"'섹션TV' 김해숙 \"\"'허스토리' 촬영 후 우울증 얻었다\"\"\"   \n",
       "4  \"[단독] 임현주 아나운서 “‘노브라 챌린지’ 방송 덕에 낸 용기, 자연스런 논의의...   \n",
       "\n",
       "                                             comment    bias  hate  label  \n",
       "0                                     김태리 정말 연기잘해 진짜    none  none      0  \n",
       "1                           공효진 발연기나이질생각이읍던데 왜계속주연일까    none  hate      1  \n",
       "2  누구처럼 돈만 밝히는 저급인생은 살아가지마시길~~ 행복은 머니순이 아니니깐 작은거에...  others  hate      3  \n",
       "3                                           일본 축구 져라    none  none      0  \n",
       "4                         난 절대로 임현주 욕하는인간이랑은 안논다 @.@    none  none      0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = []\n",
    "for i, arr in enumerate(bias_hate):\n",
    "    for idx, elem in enumerate(combinations):\n",
    "        if np.array_equal(elem, arr):\n",
    "            labels.append(idx)\n",
    "\n",
    "train_df['label'] = labels\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9352b3-de73-41fa-aa23-ae207185fb97",
   "metadata": {},
   "source": [
    "# 4. Dataset 로드 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b48556-007c-4c4c-9d53-ac9432d1d9fc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fe20b2f-09f3-4033-947a-2d8c54a4f4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.json 에서 지정 이름별로 가져올 라이브러리 지정\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "TOKENIZER_CLASSES = {\n",
    "    \"BertTokenizer\": BertTokenizer,\n",
    "    \"AutoTokenizer\": AutoTokenizer,\n",
    "    \"ElectraTokenizer\": ElectraTokenizer,\n",
    "    \"AlbertTokenizer\": AlbertTokenizer,\n",
    "    \"AutoTokenizer\": AutoTokenizer\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfdf9295-8b22-439a-abad-c8d6b4d6b65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='beomi/KcELECTRA-base', vocab_size=50135, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "TOKENIZER = TOKENIZER_CLASSES[args.tokenizer_class].from_pretrained(args.pretrained_model)\n",
    "if DEBUG==True:\n",
    "    print(TOKENIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24fa0128-a6ff-4e3d-8a7a-578dd4660311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example :  \"'미스터 션샤인' 변요한, 김태리와 같은 양복 입고 학당 방문! 이유는?\"\n",
      "comment_ex :  김태리 정말 연기잘해 진짜\n",
      "{'input_ids': [2, 6, 11, 24787, 2089, 5146, 4028, 11, 1709, 4071, 4069, 16, 20778, 4177, 4331, 8069, 35054, 12634, 47185, 13182, 5, 10491, 33, 6, 3, 20778, 4177, 8057, 11061, 4479, 4025, 7997, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "if DEBUG == True:\n",
    "    example = train_df['title'][0]\n",
    "    print('example : ', example)\n",
    "    \n",
    "    comment_ex = train_df['comment'][0]\n",
    "    print('comment_ex : ', comment_ex)\n",
    "    \n",
    "    print(TOKENIZER(example, comment_ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "961fdd49-04bb-4a78-89df-a34a737901b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode :  [2, 6, 11, 24787, 2089, 5146, 4028, 11, 1709, 4071, 4069, 16, 20778, 4177, 4331, 8069, 35054, 12634, 47185, 13182, 5, 10491, 33, 6, 3] \n",
      "\n",
      "tokenize :  ['\"', \"'\", '미스터', '션', '##샤', '##인', \"'\", '변', '##요', '##한', ',', '김태', '##리', '##와', '같은', '양복', '입고', '학당', '방문', '!', '이유는', '?', '\"'] \n",
      "\n",
      "convert tokens to ids :  ['\"', \"'\", '미스터', '션', '##샤', '##인', \"'\", '변', '##요', '##한', ',', '김태', '##리', '##와', '같은', '양복', '입고', '학당', '방문', '!', '이유는', '?', '\"']\n"
     ]
    }
   ],
   "source": [
    "if DEBUG == True:\n",
    "    print('encode : ', TOKENIZER.encode(example), \"\\n\")\n",
    "    \n",
    "    print('tokenize : ', TOKENIZER.tokenize(example), \"\\n\")\n",
    "    \n",
    "    print('convert tokens to ids : ', TOKENIZER.tokenize(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c52b63-34b3-47de-a972-4c809ee7452c",
   "metadata": {},
   "source": [
    "## 4-1. 데이터 정제 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "273e7e4c-7325-4b29-8fcd-b92968c75066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "\n",
    "emojis = ''.join(emoji.UNICODE_EMOJI.keys())\n",
    "pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-ㅣ가-힣{emojis}]+')\n",
    "url_pattern = re.compile(\n",
    "    r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "\n",
    "def clean_text(x):\n",
    "    x = pattern.sub(' ', x)\n",
    "    x = url_pattern.sub('', x)\n",
    "    x = x.strip()\n",
    "    x = repeat_normalize(x, num_repeats=2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9711eef-09f4-4a71-b8a3-d76429fe0b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'미스터 션샤인' 변요한, ㅋㅋ 김태리와 같은 양복 입고 학당 방문! 이유는?????\n"
     ]
    }
   ],
   "source": [
    "x = clean_text(\"'미스터 션샤인' 변요한, ㅋㅋㅋㅋㅋ     김태리와 같은 양복 입고 학당 방문! 이유는?????\")\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390930bb-f1ce-4415-909a-12f031986255",
   "metadata": {},
   "source": [
    "## 4-1. Dataset 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5a12b29-d8b6-4eb7-9520-2443db384f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len, mode='train', clean=None):\n",
    "        self.data = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.mode = mode\n",
    "        self.clean = clean\n",
    "        \n",
    "        if self.mode != 'test':\n",
    "            try:\n",
    "                self.labels = df['label'].tolist()\n",
    "            except:\n",
    "                assert False, \"CustomDataset Error! : \\'label\\' columns does not exist in the dataframe. check mode is not train\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        title = self.data.title.iloc[idx]\n",
    "        comment = self.data.comment.iloc[idx]\n",
    "        \n",
    "        if self.clean is not None:\n",
    "            title = self.clean(title)\n",
    "            comment = self.clean(comment)\n",
    "        \n",
    "        \n",
    "        tokenized_text = self.tokenizer(title,\n",
    "                                       comment,\n",
    "                                       padding='max_length',\n",
    "                                       max_length=self.max_len,\n",
    "                                       truncation=True,\n",
    "                                       return_token_type_ids=True,\n",
    "                                       return_attention_mask=True,\n",
    "                                       return_tensors='pt')\n",
    "        data = {\n",
    "            'input_ids': tokenized_text['input_ids'].clone().detach().long(),\n",
    "            'attention_mask': tokenized_text['attention_mask'].clone().detach().long(),\n",
    "            'token_type_ids': tokenized_text['token_type_ids'].clone().detach().long()\n",
    "        }\n",
    "        \n",
    "        if self.mode != 'test':\n",
    "            label = self.data.label.iloc[idx]\n",
    "            return data, label\n",
    "        else:\n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea062a81-2de9-4db1-85f6-78df9bb8c6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset loaded.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset(df=train_df, tokenizer=TOKENIZER, max_len=args.max_seq_len, mode='train', clean=clean_text)\n",
    "print(\"train dataset loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60aaef24-28a8-47b9-9d42-04f769a62e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset sample : \n",
      "({'input_ids': tensor([[    2,     6,    11, 24787,  2089,  5146,  4028,    11,  1709,  4071,\n",
      "          4069,    16, 20778,  4177,  4331,  8069, 35054, 12634, 47185, 13182,\n",
      "             5, 10491,    33,     6,     3, 20778,  4177,  8057, 11061,  4479,\n",
      "          4025,  7997,     3,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}, 0)\n"
     ]
    }
   ],
   "source": [
    "if DEBUG == True:\n",
    "    print('dataset sample : ')\n",
    "    print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db9f1a8-ef12-4150-b8fe-50aa0d427d2d",
   "metadata": {},
   "source": [
    "## 4-3. Train/Validation set 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "66e8f1b5-472b-40da-9d78-2f5d65327eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset:  7530\n",
      "Validation dataset:  837\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "                                                         \n",
    "train_data, val_data = train_test_split(train_df, test_size=0.1, random_state=args.seed)\n",
    "\n",
    "train_dataset = CustomDataset(train_data, TOKENIZER, args.max_seq_len, 'train', clean_text)\n",
    "val_dataset = CustomDataset(val_data, TOKENIZER, args.max_seq_len, 'validation', clean_text)\n",
    "\n",
    "print(\"Train dataset: \", len(train_dataset))\n",
    "print(\"Validation dataset: \", len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f266f7e5-342f-446f-9fcf-6185cffbe566",
   "metadata": {},
   "source": [
    "# 5. 분류 모델 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1fb2f6cc-15cc-45a7-bdf6-cc32338bc824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "BASE_MODEL = {\n",
    "    \"BertForSequenceClassification\": BertForSequenceClassification,\n",
    "    \"AutoModel\": AutoModel,\n",
    "    \"ElectraForSequenceClassification\": ElectraForSequenceClassification,\n",
    "    \"AlbertForSequenceClassification\": AlbertForSequenceClassification,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "864b71b0-8078-4e1b-8ed1-dcf582419041",
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel = BASE_MODEL[args.architecture].from_pretrained(args.pretrained_model,\n",
    "                                                       num_labels = args.num_classes,\n",
    "                                                       output_attentions=False, \n",
    "                                                       output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47b13732-7d4c-42b7-a42f-d1bf22a8c14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElectraForSequenceClassification(\n",
      "  (electra): ElectraModel(\n",
      "    (embeddings): ElectraEmbeddings(\n",
      "      (word_embeddings): Embedding(50135, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): ElectraEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): ElectraClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if DEBUG == True:\n",
    "    print(myModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f654e155-288b-429c-acef-8fdf140f34f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### v2 에서 일부 수정됨\n",
    "class myClassifier(nn.Module):\n",
    "    def __init__(self, model, hidden_size = 768, num_classes=args.num_classes, selected_layers=False, params=None):\n",
    "        super(myClassifier, self).__init__()\n",
    "        self.model = model\n",
    "        self.softmax = nn.Softmax(dim=1) \n",
    "        self.selected_layers = selected_layers\n",
    "        \n",
    "        # 사실 dr rate은 model config 에서 hidden_dropout_prob로 가져와야 하는데 bert에선 0.1이 쓰였음\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "\n",
    "    def forward(self, token_ids, attention_mask, segment_ids):      \n",
    "        outputs = self.model(input_ids = token_ids, \n",
    "                             token_type_ids = segment_ids.long(), \n",
    "                             attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        \n",
    "        # hidden state에서 마지막 4개 레이어를 뽑아 합쳐 새로운 pooled output 을 만드는 시도\n",
    "        if self.selected_layers == True:\n",
    "            hidden_states = outputs.hidden_states\n",
    "            pooled_output = torch.cat(tuple([hidden_states[i] for i in [-4, -3, -2, -1]]), dim=-1)\n",
    "            # print(\"concatenated output shape: \", pooled_output.shape)\n",
    "            ## dim(batch_size, max_seq_len, hidden_dim) 에서 가운데를 0이라 지정함으로, [cls] 토큰의 임베딩을 가져온다. \n",
    "            ## (text classification 구조 참고)\n",
    "            pooled_output = pooled_output[:, 0, :]\n",
    "            # print(pooled_output)\n",
    "\n",
    "            pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "            ## 3개의 레이어를 합치므로 classifier의 차원은 (hidden_dim, 6)이다\n",
    "            classifier = nn.Linear(pooled_output.shape[1], args.num_classes).to(token_ids.device)\n",
    "            logits = classifier(pooled_output)\n",
    "        \n",
    "        else:\n",
    "            logits=outputs.logits\n",
    "        \n",
    "    \n",
    "        # 각 클래스별 확률\n",
    "        prob= self.softmax(logits)\n",
    "        # print(prob)\n",
    "        # logits2 = outputs.logits\n",
    "        # print(self.softmax(logits2))\n",
    "\n",
    "\n",
    "        return logits, prob\n",
    "        \n",
    "# 마지막 4 hidden layers concat 하는 방법을 쓰신다면 True로 변경        \n",
    "model = myClassifier(myModel, selected_layers=False)\n",
    "\n",
    "# if DEBUG ==True :\n",
    "#     print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a40bb3c0-53de-44f8-b2e8-4fad9e608ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "model.electra.embeddings.word_embeddings.weight         (50135, 768)\n",
      "model.electra.embeddings.position_embeddings.weight       (512, 768)\n",
      "model.electra.embeddings.token_type_embeddings.weight       (2, 768)\n",
      "model.electra.embeddings.LayerNorm.weight                     (768,)\n",
      "model.electra.embeddings.LayerNorm.bias                       (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "model.electra.encoder.layer.0.attention.self.query.weight   (768, 768)\n",
      "model.electra.encoder.layer.0.attention.self.query.bias       (768,)\n",
      "model.electra.encoder.layer.0.attention.self.key.weight   (768, 768)\n",
      "model.electra.encoder.layer.0.attention.self.key.bias         (768,)\n",
      "model.electra.encoder.layer.0.attention.self.value.weight   (768, 768)\n",
      "model.electra.encoder.layer.0.attention.self.value.bias       (768,)\n",
      "model.electra.encoder.layer.0.attention.output.dense.weight   (768, 768)\n",
      "model.electra.encoder.layer.0.attention.output.dense.bias       (768,)\n",
      "model.electra.encoder.layer.0.attention.output.LayerNorm.weight       (768,)\n",
      "model.electra.encoder.layer.0.attention.output.LayerNorm.bias       (768,)\n",
      "model.electra.encoder.layer.0.intermediate.dense.weight  (3072, 768)\n",
      "model.electra.encoder.layer.0.intermediate.dense.bias        (3072,)\n",
      "model.electra.encoder.layer.0.output.dense.weight        (768, 3072)\n",
      "model.electra.encoder.layer.0.output.dense.bias               (768,)\n",
      "model.electra.encoder.layer.0.output.LayerNorm.weight         (768,)\n",
      "model.electra.encoder.layer.0.output.LayerNorm.bias           (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "model.classifier.dense.weight                             (768, 768)\n",
      "model.classifier.dense.bias                                   (768,)\n",
      "model.classifier.out_proj.weight                            (6, 768)\n",
      "model.classifier.out_proj.bias                                  (6,)\n"
     ]
    }
   ],
   "source": [
    "if DEBUG==True:\n",
    "    params = list(model.named_parameters())\n",
    "\n",
    "    print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "    print('==== Embedding Layer ====\\n')\n",
    "\n",
    "    for p in params[0:5]:\n",
    "        print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "    print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "    for p in params[5:21]:\n",
    "        print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "    print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "    for p in params[-4:]:\n",
    "        print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e4bed5-7a2c-4c95-9a77-8a85bc75078d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6. 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f09047b-bcd0-433c-ae66-e01a14e3f117",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6-1. Early Stopper 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fbbd87f7-f38d-4ba4-955f-ecd4829cc040",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossEarlyStopper():\n",
    "    \"\"\"Early stopper\n",
    "\n",
    "        patience (int): loss가 줄어들지 않아도 학습할 epoch 수\n",
    "        patience_counter (int): loss 가 줄어들지 않을 때 마다 1씩 증가\n",
    "        min_loss (float): 최소 loss\n",
    "        stop (bool): True 일 때 학습 중단\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience: int)-> None:\n",
    "        \"\"\" 초기화\n",
    "\n",
    "        Args:\n",
    "            patience (int): loss가 줄어들지 않아도 학습할 epoch 수\n",
    "            weight_path (str): weight 저장경로\n",
    "            verbose (bool): 로그 출력 여부, True 일 때 로그 출력\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.patience_counter = 0\n",
    "        self.min_loss = np.Inf\n",
    "        self.stop = False\n",
    "\n",
    "    def check_early_stopping(self, loss: float)-> None:\n",
    "        # 첫 에폭\n",
    "        if self.min_loss == np.Inf:\n",
    "            self.min_loss = loss\n",
    "           \n",
    "        # loss가 줄지 않는다면 -> patience_counter 1 증가\n",
    "        elif loss > self.min_loss:\n",
    "            self.patience_counter += 1\n",
    "            msg = f\"Early stopping counter {self.patience_counter}/{self.patience}\"\n",
    "\n",
    "            # patience 만큼 loss가 줄지 않았다면 학습을 중단합니다.\n",
    "            if self.patience_counter == self.patience:\n",
    "                self.stop = True\n",
    "            print(msg)\n",
    "        # loss가 줄어듬 -> min_loss 갱신, patience_counter 초기화\n",
    "        elif loss <= self.min_loss:\n",
    "            self.patience_counter = 0\n",
    "            ### v2 에서 수정됨\n",
    "            ### self.save_model = True -> 삭제 (사용하지 않음)\n",
    "            msg = f\"Validation loss decreased {self.min_loss} -> {loss}\"\n",
    "            self.min_loss = loss\n",
    "\n",
    "            print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25846a7-465c-490b-8eb1-21b075adafc0",
   "metadata": {},
   "source": [
    "## 6-2. Epoch 별 학습 및 검증"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f11186c-a8b8-48cb-b3b7-7df3085e125a",
   "metadata": {},
   "source": [
    "- [Transformers optimization documentation](https://huggingface.co/docs/transformers/main_classes/optimizer_schedules)\n",
    "- [스케줄러 documentation](https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#schedules)\n",
    "- Adam optimizer의 epsilon 파라미터 eps = 1e-8 는 \"계산 중 0으로 나눔을 방지 하기 위한 아주 작은 숫자 \" 입니다. ([출처](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/))\n",
    "- 스케줄러 파라미터\n",
    "    - `warmup_ratio` : \n",
    "      - 학습이 진행되면서 학습률을 그 상황에 맞게 가변적으로 적당하게 변경되게 하기 위해 Scheduler를 사용합니다.\n",
    "      - 처음 학습률(Learning rate)를 warm up하기 위한 비율을 설정하는 warmup_ratio을 설정합니다.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "981e7e84-fd78-427c-bf4f-75ebe5f703c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config file loaded\n",
      "beomi/KcELECTRA-base\n",
      "RUN :  demo3\n",
      "batch size :  20\n",
      "The first batch looks like ..\n",
      " {'input_ids': tensor([[[    2,     6,    11,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    61,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    61,  ...,     0,     0,     0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[    2,     6, 29663,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    11,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,    11,   163,  ...,     0,     0,     0]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]]]), 'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 0/377 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[[    2,     6,  2484,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    61,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    11,  ...,     0,     0,     0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[    2,     6, 27805,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,   591,  8895,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    61,  ...,     0,     0,     0]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]]]), 'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]]])} tensor([5, 3, 3, 0, 5, 1, 0, 5, 0, 0, 0, 0, 0, 5, 0, 0, 5, 3, 3, 0])\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 1/377 [00:00<05:48,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[[    2,     6, 32314,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,     6,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6, 30483,  ...,     0,     0,     0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[    2,     6,    11,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    11,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,     6,  ...,     0,     0,     0]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]]]), 'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]]])} tensor([5, 1, 0, 0, 5, 0, 2, 0, 2, 3, 3, 5, 1, 1, 0, 0, 3, 3, 0, 0])\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1% 2/377 [00:01<04:51,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[[    2,     6,     6,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    11,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,     6,  ...,     0,     0,     0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[    2,     6, 36275,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,     6,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6, 19000,  ...,     0,     0,     0]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]]]), 'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]]])} tensor([5, 0, 3, 0, 5, 0, 1, 1, 0, 0, 1, 0, 0, 3, 3, 5, 1, 1, 0, 1])\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1% 3/377 [00:01<04:12,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[[    2,     6, 10657,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    11,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6, 31241,  ...,     0,     0,     0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[    2,     6,  9319,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,    61, 18398,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    61,  ...,     0,     0,     0]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]]]), 'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]]])} tensor([0, 0, 1, 2, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 3, 3, 1, 5, 5, 0])\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1% 4/377 [00:02<03:43,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[[    2,     6,    61,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,   264,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    11,  ...,     0,     0,     0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[    2,     6,    61,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2, 31610,    97,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2, 37338,  4486,  ...,     0,     0,     0]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]]]), 'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]]])} tensor([1, 4, 0, 0, 3, 0, 5, 5, 1, 1, 0, 3, 1, 5, 0, 0, 0, 3, 5, 5])\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1% 5/377 [00:02<03:23,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[[    2,     6, 29573,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6, 15877,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    61,  ...,     0,     0,     0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[    2,     6,    61,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    61,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    61,  ...,     0,     0,     0]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]]]), 'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]]])} tensor([1, 3, 0, 1, 0, 3, 1, 1, 5, 5, 3, 0, 5, 0, 5, 0, 0, 1, 3, 1])\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2% 6/377 [00:03<03:10,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[[    2,     6, 37150,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2, 34525,  4608,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    61,  ...,     0,     0,     0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[    2,     6,  3222,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    61,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    11,  ...,     0,     0,     0]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]]]), 'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]]])} tensor([1, 5, 5, 0, 3, 0, 0, 1, 0, 1, 0, 0, 0, 3, 1, 5, 0, 0, 0, 0])\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2% 7/377 [00:03<03:00,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[[    2,     6, 17010,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6, 31760,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,  3777,  ...,     0,     0,     0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[    2,     6, 19445,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,   602,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    61,  ...,     0,     0,     0]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]]]), 'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]]])} tensor([0, 1, 2, 1, 0, 0, 0, 3, 1, 0, 0, 1, 3, 0, 1, 1, 0, 5, 1, 1])\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2% 8/377 [00:03<02:54,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[[    2,     6, 48875,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6, 27899,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6, 44032,  ...,     0,     0,     0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[    2,     6,     6,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6, 30043,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    11,  ...,     0,     0,     0]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]]]), 'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]]])} tensor([0, 5, 3, 0, 5, 0, 0, 1, 1, 1, 0, 1, 3, 5, 5, 5, 0, 3, 1, 3])\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2% 9/377 [00:04<02:48,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[[    2,     6,    61,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2, 33078,    97,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,  8899,  ...,     0,     0,     0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[    2,     6, 40010,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    11,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    61,  ...,     0,     0,     0]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]]]), 'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]]])} tensor([3, 3, 0, 0, 0, 0, 0, 1, 1, 3, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0])\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3% 10/377 [00:04<02:45,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[[    2,     6,    61,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    11,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    11,  ...,     0,     0,     0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[    2,     6,  2693,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6, 13857,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    61,  ...,     0,     0,     0]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]]]), 'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]]])} tensor([3, 0, 0, 3, 1, 5, 0, 3, 5, 0, 3, 0, 1, 0, 1, 0, 5, 3, 0, 3])\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3% 11/377 [00:05<02:43,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[[    2,     6,  2484,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6, 35348,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6, 38845,  ...,     0,     0,     0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[    2,     6,     6,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    61,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,    61,   149,  ...,     0,     0,     0]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]]]), 'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]]])} tensor([3, 1, 3, 0, 3, 1, 0, 0, 0, 1, 3, 5, 5, 1, 3, 5, 0, 0, 1, 0])\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3% 12/377 [00:05<02:40,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[[   2,    6,   61,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[   2,    6,    6,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[   2,    6,   61,  ...,    0,    0,    0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[   2,    6,   61,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[   2, 9223, 8173,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[   2,    6,   11,  ...,    0,    0,    0]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]]]), 'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]]])} tensor([1, 0, 0, 0, 0, 3, 1, 1, 0, 3, 3, 1, 0, 1, 0, 3, 3, 3, 4, 0])\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3% 13/377 [00:06<02:40,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[[    2,     6,    11,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    61,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    11,  ...,     0,     0,     0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[    2,     6,    11,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6, 43321,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    61,  ...,     0,     0,     0]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]]]), 'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]]])} tensor([1, 5, 0, 1, 5, 0, 5, 1, 0, 3, 0, 3, 0, 1, 0, 0, 0, 0, 1, 5])\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4% 14/377 [00:06<02:39,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[[    2,     6,     6,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,  2571,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    61,  ...,     0,     0,     0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[    2,     6,    11,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    61,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2, 47354,    97,  ...,     0,     0,     0]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]]]), 'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]]])} tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 5, 3, 0, 4, 1, 1, 1])\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4% 15/377 [00:06<02:37,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[[    2,     6, 45423,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    61,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    61,  ...,     0,     0,     0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[    2,    61, 18398,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,  2438,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2, 29734,  4138,  ...,     0,     0,     0]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]]]), 'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]]])} tensor([0, 0, 1, 5, 3, 0, 1, 0, 0, 1, 1, 3, 0, 5, 1, 1, 0, 3, 0, 0])\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4% 16/377 [00:07<02:47,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[[    2,    11,  8320,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    61,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,  2111,  ...,     0,     0,     0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[    2,     6,     6,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,    61, 15085,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[    2,     6,    11,  ...,     0,     0,     0]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]]]), 'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]]])} tensor([1, 1, 3, 3, 1, 3, 0, 4, 3, 5, 0, 1, 3, 1, 5, 0, 3, 5, 5, 3])\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5521/1620783826.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_5521/1620783826.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data, val_data, args, mode)\u001b[0m\n\u001b[1;32m     71\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m                     \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                     \u001b[0mtotal_loss_train\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5521/696126354.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, token_ids, attention_mask, segment_ids)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         outputs = self.model(input_ids = token_ids, \n\u001b[0m\u001b[1;32m     15\u001b[0m                              \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                              attention_mask = attention_mask.float().to(token_ids.device))\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/electra/modeling_electra.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    998\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1000\u001b[0;31m         discriminator_hidden_states = self.electra(\n\u001b[0m\u001b[1;32m   1001\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/electra/modeling_electra.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    914\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings_project\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m         hidden_states = self.encoder(\n\u001b[0m\u001b[1;32m    917\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/electra/modeling_electra.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    582\u001b[0m                 )\n\u001b[1;32m    583\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    585\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/electra/modeling_electra.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    471\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/electra/modeling_electra.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     ):\n\u001b[0;32m--> 397\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/electra/modeling_electra.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     ):\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mmixed_query_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;31m# If this is instantiated as a cross-attention module, the keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1692\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1694\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1695\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = set_config(config_path)\n",
    "\n",
    "logging.set_verbosity_warning()\n",
    "\n",
    "# 재현을 위해 모든 곳의 시드 고정\n",
    "seed_val = args.seed\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "def train(model, train_data, val_data, args, mode = 'train'):\n",
    "    \n",
    "    # args.run은 실험 이름 (어디까지나 팀원들간의 버전 관리 및 공유 편의를 위한 것으로, 자유롭게 수정 가능합니다.)\n",
    "    print(\"RUN : \", args.run)\n",
    "    shutil.copyfile(\"config/config.json\", os.path.join(args.config_dir, f\"config_{args.run}.json\"))\n",
    "\n",
    "    early_stopper = LossEarlyStopper(patience=args.patience)\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=args.train_batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_data, batch_size=args.train_batch_size)\n",
    "\n",
    "    \n",
    "    if DEBUG == True:\n",
    "        # 데이터로더가 성공적으로 로드 되었는지 확인\n",
    "        for idx, data in enumerate(train_dataloader):\n",
    "            if idx==0:\n",
    "                print(\"batch size : \", len(data[0]['input_ids']))\n",
    "                print(\"The first batch looks like ..\\n\", data[0])\n",
    "    \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    total_steps = len(train_dataloader) * args.train_epochs\n",
    "\n",
    "    ### v2에서 수정됨 (Adam -> AdamW)\n",
    "    optimizer = AdamW(model.parameters(), lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(total_steps * args.warmup_proportion), \n",
    "                                                num_training_steps=total_steps)\n",
    "\n",
    "    \n",
    "    if use_cuda:\n",
    "        model = model.to(DEVICE)\n",
    "        criterion = criterion.to(DEVICE)\n",
    "        \n",
    "\n",
    "    tr_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    best_score = 0.0\n",
    "    best_loss = np.inf\n",
    "      \n",
    "\n",
    "    for epoch_num in range(args.train_epochs):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "            \n",
    "            assert mode in ['train', 'val'], 'your mode should be either \\'train\\' or \\'val\\''\n",
    "            \n",
    "            if mode =='train':\n",
    "                for train_input, train_label in tqdm(train_dataloader):\n",
    "                    print(train_input, train_label)\n",
    "                    print(type(train_label))\n",
    "                    \n",
    "                    mask = train_input['attention_mask'].to(DEVICE)\n",
    "                    input_id = train_input['input_ids'].squeeze(1).to(DEVICE)\n",
    "                    segment_ids = train_input['token_type_ids'].squeeze(1).to(DEVICE)\n",
    "                    train_label = train_label.long().to(DEVICE)  \n",
    "                    \n",
    "                    ### v2에 수정됨\n",
    "                    optimizer.zero_grad()\n",
    " \n",
    "                    output = model(input_id, mask, segment_ids)\n",
    "                    batch_loss = criterion(output[0].view(-1,6), train_label.view(-1))\n",
    "                    total_loss_train += batch_loss.item()\n",
    "\n",
    "                    acc = (output[0].argmax(dim=1) == train_label).sum().item()\n",
    "                    total_acc_train += acc\n",
    "                    \n",
    "                    ### v2에 수정됨\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    batch_loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    ### v2 에 수정됨\n",
    "                    scheduler.step()\n",
    "                    \n",
    "\n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "            \n",
    "            # validation을 위해 이걸 넣으면 이 evaluation 프로세스 중엔 dropout 레이어가 다르가 동작한다.\n",
    "            model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    mask = val_input['attention_mask'].to(DEVICE)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(DEVICE)\n",
    "                    segment_ids = val_input['token_type_ids'].squeeze(1).to(DEVICE)\n",
    "                    val_label = val_label.long().to(DEVICE)\n",
    "\n",
    "                    output = model(input_id, mask, segment_ids)\n",
    "                    ### v2 에서 일부 수정 (output -> output[0]로 myClassifier 모델에 정의된대로 logits 가져옴)\n",
    "                    batch_loss = criterion(output[0].view(-1,6), val_label.view(-1))\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    ### v2 에서 일부 수정 (output -> output[0]로 myClassifier 모델에 정의된대로 logits 가져옴)\n",
    "                    acc = (output[0].argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            \n",
    "            train_loss = total_loss_train / len(train_data)\n",
    "            train_accuracy = total_acc_train / len(train_data)\n",
    "            val_loss = total_loss_val / len(val_data)\n",
    "            val_accuracy = total_acc_val / len(val_data)\n",
    "            \n",
    "            # 한 Epoch 학습 후 학습/검증에 대해 loss와 평가지표 (여기서는 accuracy로 임의로 설정) 출력\n",
    "            print(\n",
    "                f'Epoch: {epoch_num + 1} \\\n",
    "                | Train Loss: {train_loss: .3f} \\\n",
    "                | Train Accuracy: {train_accuracy: .3f} \\\n",
    "                | Val Loss: {val_loss: .3f} \\\n",
    "                | Val Accuracy: {val_accuracy: .3f}')\n",
    "          \n",
    "            # early_stopping check\n",
    "            early_stopper.check_early_stopping(loss=val_loss)\n",
    "\n",
    "            if early_stopper.stop:\n",
    "                print('Early stopped, Best score : ', best_score)\n",
    "                break\n",
    "\n",
    "            ### v2 에 수정됨\n",
    "            ### loss와 accuracy가 꼭 correlate하진 않습니다.\n",
    "            ### \n",
    "            ### 원본 (필요하다면 다시 해제 후 사용)\n",
    "            # if val_accuracy > best_score : \n",
    "            if val_loss < best_loss :\n",
    "            # 모델이 개선됨 -> 검증 점수와 베스트 loss, weight 갱신\n",
    "                best_score = val_accuracy \n",
    "                \n",
    "                ### v2에서 추가\n",
    "                best_loss =val_loss\n",
    "                # 학습된 모델을 저장할 디렉토리 및 모델 이름 지정\n",
    "                SAVED_MODEL =  os.path.join(args.result_dir, f'best_{args.run}.pt')\n",
    "            \n",
    "                check_point = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'scheduler': scheduler.state_dict()\n",
    "                }\n",
    "                torch.save(check_point, SAVED_MODEL)  \n",
    "              \n",
    "            # print(\"scheduler : \", scheduler.state_dict())\n",
    "\n",
    "\n",
    "    print(\"train finished\")\n",
    "\n",
    "\n",
    "train(model, train_dataset, val_dataset, args, mode = 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dfb418-65bf-42c1-a455-6ca3d09e9025",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 7. Test dataset으로 추론 (Prediction)\n",
    "\n",
    "\n",
    "- v2 에서 수정된 부분\n",
    "    - output -> output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ff388b-2609-4f5d-afcc-4470c766d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 테스트 데이터셋 불러오기\n",
    "test_data = CustomDataset(test_df, tokenizer = TOKENIZER, max_len= args.max_seq_len, mode='test')\n",
    "\n",
    "def test(model, SAVED_MODEL, test_data, args, mode = 'test'):\n",
    "\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=args.eval_batch_size)\n",
    "\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.to(DEVICE)\n",
    "        model.load_state_dict(torch.load(SAVED_MODEL)['model'])\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_input in test_dataloader:\n",
    "\n",
    "            mask = test_input['attention_mask'].to(DEVICE)\n",
    "            input_id = test_input['input_ids'].squeeze(1).to(DEVICE)\n",
    "            segment_ids = test_input['token_type_ids'].squeeze(1).to(DEVICE)\n",
    "\n",
    "            output = model(input_id, mask, segment_ids)\n",
    "\n",
    "            output = output[0].argmax(dim=1).cpu().tolist()\n",
    "\n",
    "            for label in output:\n",
    "                pred.append(label)\n",
    "                \n",
    "    return pred\n",
    "\n",
    "SAVED_MODEL =  os.path.join(args.result_dir, f'best_{args.run}.pt')\n",
    "\n",
    "pred = test(model, SAVED_MODEL, test_data, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e65d372-2a25-4388-9ef7-64ca37ef8b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"prediction completed for \", len(pred), \"comments\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01d40ee-eb4c-4725-a942-761abe39a1da",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd457b44-1d05-43d9-9740-91023c9dc3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-5 사이의 라벨 값 별로 bias, hate로 디코딩 하기 위한 딕셔너리\n",
    "bias_dict = {0: 'none', 1: 'none', 2: 'others', 3:'others', 4:'gender', 5:'gender'}\n",
    "hate_dict = {0: 'none', 1: 'hate', 2: 'none', 3:'hate', 4:'none', 5:'hate'}\n",
    "\n",
    "# 인코딩 값으로 나온 타겟 변수를 디코딩\n",
    "pred_bias = ['' for i in range(len(pred))]\n",
    "pred_hate = ['' for i in range(len(pred))]\n",
    "\n",
    "for idx, label in enumerate(pred):\n",
    "    pred_bias[idx]=(str(bias_dict[label]))\n",
    "    pred_hate[idx]=(str(hate_dict[label]))\n",
    "print('decode Completed!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad942bf-a39b-4a61-ad52-4efe253a0d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv(os.path.join(args.data_dir,'sample_submission.csv'))\n",
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c687ee-d7d1-4f0e-a67f-60dea31d2632",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit['bias'] = pred_bias\n",
    "submit['hate'] = pred_hate\n",
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59490788-ab94-4224-927d-4b0870a53ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv(os.path.join(args.result_dir, f\"submission_{args.run}.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
